■ 全体の流れ
① ラベル付き教師データの準備
まず、クレームの文章とそれぞれの問題を示すラベル（カテゴリ）を用意する。
例：

テキスト（クレーム内容）	ラベル（カテゴリ）
商品が破損していたので返品したい	商品不良
担当者の態度が悪かった	接客態度
配達が遅れて届かなかった	配送問題
教師データ数の目安は、数百〜数千件程度以上が望ましい。
（例：500件以上推奨）

② BERTの事前学習済みモデルを準備する
日本語のテキスト分類なら、日本語向けのBERTを利用する。

主な選択肢：

cl-tohoku/bert-base-japanese（東北大学の事前学習モデル）

cl-tohoku/bert-base-japanese-whole-word-masking

Googleが公開するMultilingual BERT など

日本語なら「cl-tohoku/bert-base-japanese-whole-word-masking」が精度が高い傾向にあり、おすすめだよ。

③ BERTのファインチューニング（Fine-tuning）
事前学習済みのBERTを元に、自分の教師データを使って「ファインチューニング（微調整）」を行う。
簡単に言えば、クレームを読み込ませ、正しいラベルを予測できるように学習させる。

ここでは主にPyTorchやTransformersライブラリ（Hugging Face社）を使うのが便利。

必要なライブラリ：

bash
コピーする
編集する
pip install transformers datasets torch scikit-learn
③ 実際のコード例（シンプルな実装）
実際にコードを書くと、以下のような形になるよ。

Step-by-stepコード例（Python＋Transformers）:

python
コピーする
編集する
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# データを読み込み（CSV想定）
df = pd.read_csv('claims.csv')  # 例: "text", "label"の列
train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)

# ラベルを数値化
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_df['label'])
val_labels = label_encoder.transform(eval_df['label'])

# モデルとトークナイザーを読み込み（日本語BERT使用）
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))

# データセットの準備
class ClaimsDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = ClaimsDataset(train_df['text'].tolist(), train_labels)
val_dataset = ClaimsDataset(eval_df['text'].tolist(), val_labels)

# 学習の設定
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,          # 通常3〜5エポック程度
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch"
)

# モデル準備
trainer = Trainer(
    model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_)),
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# 学習実行
trainer.train()

# 評価
trainer.evaluate()
③ モデルの評価と運用
学習後は、精度（Accuracy）、再現率（Recall）、適合率（Precision）、F1スコアなどを評価。

分類精度が十分なら、作成したモデルを本番環境にデプロイし、運用。

精度不足の場合、データの追加やチューニングを繰り返す。

■ よくある工夫ポイント
データ量が少ない場合
転移学習やデータ拡張（Back Translationなど）で対応する。

多クラス分類の場合
クラス間のデータのバランスを取る。
サンプリングや重み付け（class_weights）で対処する。

運用時の工夫
クレームが多様で曖昧な場合、モデルが迷ったものは人間のレビューを挟むフローにすると安定運用が可能になるよ。

■ 具体的な運用事例
実際に企業では以下のようなケースで活用されている。

顧客サポートへの問い合わせメール自動分類

SNSやレビューサイトでの問題点抽出

コールセンターでの問い合わせ内容の自動整理

■ より良い精度を得るために（追加での工夫）
ファインチューニングのハイパーパラメータ調整（学習率、エポック数）

クロスバリデーションによる検証精度向上

分類結果の誤りを再学習データに活用するサイクルを構築（Active Learning）

